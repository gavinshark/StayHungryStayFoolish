"""
RAGAS Evaluator Module

Implements the RAGAS evaluation framework integration for evaluating RAG systems.
Implements Requirements 5.1, 5.2, 5.3, 5.4, 5.5, 5.6, 5.7.
"""

import json
import os
from pathlib import Path
from typing import TYPE_CHECKING, Optional

from datasets import Dataset
from ragas import evaluate
from langchain_openai import ChatOpenAI, OpenAIEmbeddings

# Import metrics from the new recommended location (ragas v1.0+)
try:
    from ragas.metrics._faithfulness import Faithfulness
    from ragas.metrics._answer_relevance import ResponseRelevancy
    from ragas.metrics._context_precision import ContextPrecision
    from ragas.metrics._context_recall import ContextRecall
    
    # Create metric instances
    faithfulness = Faithfulness()
    answer_relevancy = ResponseRelevancy()
    context_precision = ContextPrecision()
    context_recall = ContextRecall()
except ImportError:
    # Fallback to legacy imports for older ragas versions
    from ragas.metrics import (
        faithfulness,
        answer_relevancy,
        context_precision,
        context_recall,
    )

from .models import EvaluationSample, EvaluationResult

if TYPE_CHECKING:
    from .rag_chain import RAGChain


class RagasEvaluator:
    """
    RAGAS è¯„æµ‹å™¨
    
    ä½¿ç”¨ RAGAS æ¡†æ¶å¯¹ RAG ç³»ç»Ÿè¿›è¡Œå¤šç»´åº¦è¯„æµ‹ï¼ŒåŒ…æ‹¬ï¼š
    - Faithfulnessï¼ˆå¿ å®åº¦ï¼‰ï¼šç­”æ¡ˆä¸æ£€ç´¢ä¸Šä¸‹æ–‡çš„ä¸€è‡´æ€§
    - Answer Relevancyï¼ˆç­”æ¡ˆç›¸å…³æ€§ï¼‰ï¼šç­”æ¡ˆä¸é—®é¢˜çš„ç›¸å…³ç¨‹åº¦
    - Context Precisionï¼ˆä¸Šä¸‹æ–‡ç²¾ç¡®åº¦ï¼‰ï¼šæ£€ç´¢ä¸Šä¸‹æ–‡çš„ç²¾ç¡®æ€§
    - Context Recallï¼ˆä¸Šä¸‹æ–‡å¬å›ç‡ï¼‰ï¼šæ£€ç´¢ä¸Šä¸‹æ–‡çš„å®Œæ•´æ€§
    
    Attributes:
        rag_chain: RAG é“¾å®ä¾‹ï¼Œç”¨äºè·å–ç­”æ¡ˆå’Œä¸Šä¸‹æ–‡
        metrics: RAGAS è¯„æµ‹æŒ‡æ ‡åˆ—è¡¨
    """
    
    def __init__(
        self, 
        rag_chain: "RAGChain",
        api_key: Optional[str] = None,
        base_url: Optional[str] = None,
        model: str = "gpt-3.5-turbo",
        embedding_model: str = "text-embedding-v4"
    ):
        """
        åˆå§‹åŒ–è¯„æµ‹å™¨
        
        Args:
            rag_chain: RAG é“¾å®ä¾‹
            api_key: OpenAI API å¯†é’¥ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
            base_url: API Base URLï¼ˆå¯é€‰ï¼‰
            model: è¯„æµ‹ä½¿ç”¨çš„ LLM æ¨¡å‹
            embedding_model: è¯„æµ‹ä½¿ç”¨çš„ Embedding æ¨¡å‹
        """
        self.rag_chain = rag_chain
        self.metrics = [
            faithfulness,
            answer_relevancy,
            context_precision,
            context_recall,
        ]
        
        # é…ç½® RAGAS ä½¿ç”¨çš„ LLM å’Œ Embeddings
        self.api_key = api_key or os.environ.get("OPENAI_API_KEY")
        self.base_url = base_url or os.environ.get("OPENAI_BASE_URL")
        self.model = model
        self.embedding_model = embedding_model
        
        # åˆ›å»º LLM å’Œ Embeddings å®ä¾‹
        self._llm = None
        self._embeddings = None
    
    def _get_llm(self):
        """è·å–é…ç½®å¥½çš„ LLM å®ä¾‹"""
        if self._llm is None:
            kwargs = {"api_key": self.api_key, "model": self.model}
            if self.base_url:
                kwargs["base_url"] = self.base_url
            self._llm = ChatOpenAI(**kwargs)
        return self._llm
    
    def _get_embeddings(self):
        """è·å–é…ç½®å¥½çš„ Embeddings å®ä¾‹"""
        if self._embeddings is None:
            kwargs = {"api_key": self.api_key, "model": self.embedding_model}
            if self.base_url:
                kwargs["base_url"] = self.base_url
                kwargs["check_embedding_ctx_length"] = False
            self._embeddings = OpenAIEmbeddings(**kwargs)
        return self._embeddings
    
    def load_dataset(self, path: str) -> list[EvaluationSample]:
        """
        åŠ è½½è¯„æµ‹æ•°æ®é›†
        
        ä» JSON æ–‡ä»¶åŠ è½½è¯„æµ‹æ•°æ®é›†ï¼Œæ–‡ä»¶æ ¼å¼åº”ä¸ºï¼š
        {
            "samples": [
                {
                    "question": "é—®é¢˜æ–‡æœ¬",
                    "ground_truth": "å‚è€ƒç­”æ¡ˆ",
                    "contexts": ["å¯é€‰çš„å‚è€ƒä¸Šä¸‹æ–‡"]  // å¯é€‰å­—æ®µ
                }
            ]
        }
        
        Args:
            path: æ•°æ®é›†æ–‡ä»¶è·¯å¾„ (JSON æ ¼å¼)
            
        Returns:
            è¯„æµ‹æ ·æœ¬åˆ—è¡¨
            
        Raises:
            FileNotFoundError: å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨
            ValueError: å¦‚æœæ•°æ®é›†æ ¼å¼ä¸æ­£ç¡®
            
        Validates:
            - Requirement 5.1: åŠ è½½è¯„æµ‹æ•°æ®é›†
            - Requirement 5.7: æ•°æ®é›†æ ¼å¼éªŒè¯
        """
        file_path = Path(path)
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not file_path.exists():
            raise FileNotFoundError(f"Evaluation dataset file not found: {path}")
        
        # è¯»å– JSON æ–‡ä»¶
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                data = json.load(f)
        except json.JSONDecodeError as e:
            raise ValueError(f"Invalid JSON format in evaluation dataset: {e}")
        
        # éªŒè¯æ•°æ®é›†æ ¼å¼
        # Validates Requirement 5.7: æ•°æ®é›†æ ¼å¼éªŒè¯
        if not isinstance(data, dict):
            raise ValueError(
                "Invalid evaluation dataset format: expected a JSON object with 'samples' key"
            )
        
        if "samples" not in data:
            raise ValueError(
                "Invalid evaluation dataset format: missing 'samples' key"
            )
        
        samples_data = data["samples"]
        
        if not isinstance(samples_data, list):
            raise ValueError(
                "Invalid evaluation dataset format: 'samples' must be a list"
            )
        
        if len(samples_data) == 0:
            raise ValueError("Evaluation dataset is empty: 'samples' list has no items")
        
        # è§£ææ ·æœ¬
        samples = []
        for i, sample_data in enumerate(samples_data):
            if not isinstance(sample_data, dict):
                raise ValueError(
                    f"Invalid sample format at index {i}: expected a JSON object"
                )
            
            # éªŒè¯å¿…éœ€å­—æ®µ
            if "question" not in sample_data:
                raise ValueError(
                    f"Invalid sample format at index {i}: missing 'question' field"
                )
            
            if "ground_truth" not in sample_data:
                raise ValueError(
                    f"Invalid sample format at index {i}: missing 'ground_truth' field"
                )
            
            question = sample_data["question"]
            ground_truth = sample_data["ground_truth"]
            
            # éªŒè¯å­—æ®µç±»å‹
            if not isinstance(question, str) or not question.strip():
                raise ValueError(
                    f"Invalid sample format at index {i}: 'question' must be a non-empty string"
                )
            
            if not isinstance(ground_truth, str) or not ground_truth.strip():
                raise ValueError(
                    f"Invalid sample format at index {i}: 'ground_truth' must be a non-empty string"
                )
            
            # è§£æå¯é€‰çš„ contexts å­—æ®µ
            contexts = sample_data.get("contexts")
            if contexts is not None:
                if not isinstance(contexts, list):
                    raise ValueError(
                        f"Invalid sample format at index {i}: 'contexts' must be a list"
                    )
                for j, ctx in enumerate(contexts):
                    if not isinstance(ctx, str):
                        raise ValueError(
                            f"Invalid sample format at index {i}: 'contexts[{j}]' must be a string"
                        )
            
            # åˆ›å»º EvaluationSample
            sample = EvaluationSample(
                question=question,
                ground_truth=ground_truth,
                contexts=contexts,
            )
            samples.append(sample)
        
        return samples
    
    def prepare_evaluation_data(self, samples: list[EvaluationSample]) -> Dataset:
        """
        å‡†å¤‡ RAGAS è¯„æµ‹æ•°æ®
        
        å¯¹æ¯ä¸ªæ ·æœ¬è°ƒç”¨ RAG é“¾è·å–ç­”æ¡ˆå’Œä¸Šä¸‹æ–‡ï¼Œ
        æ„å»º RAGAS æ‰€éœ€çš„ Dataset æ ¼å¼ã€‚
        
        RAGAS éœ€è¦çš„æ•°æ®æ ¼å¼ï¼š
        - question: é—®é¢˜
        - answer: RAG ç³»ç»Ÿç”Ÿæˆçš„ç­”æ¡ˆ
        - contexts: æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡åˆ—è¡¨
        - ground_truth: å‚è€ƒç­”æ¡ˆ
        
        Args:
            samples: è¯„æµ‹æ ·æœ¬åˆ—è¡¨
            
        Returns:
            HuggingFace Dataset å¯¹è±¡
            
        Raises:
            ValueError: å¦‚æœæ ·æœ¬åˆ—è¡¨ä¸ºç©º
        """
        if not samples:
            raise ValueError("Cannot prepare evaluation data: samples list is empty")
        
        # å‡†å¤‡ RAGAS æ‰€éœ€çš„æ•°æ®ç»“æ„
        questions = []
        answers = []
        contexts = []
        ground_truths = []
        
        for sample in samples:
            # è°ƒç”¨ RAG é“¾è·å–ç­”æ¡ˆå’Œä¸Šä¸‹æ–‡
            response = self.rag_chain.query(sample.question)
            
            questions.append(sample.question)
            answers.append(response.answer)
            contexts.append(response.contexts)
            ground_truths.append(sample.ground_truth)
        
        # æ„å»º HuggingFace Dataset
        # RAGAS éœ€è¦çš„åˆ—åï¼šquestion, answer, contexts, ground_truth
        dataset = Dataset.from_dict({
            "question": questions,
            "answer": answers,
            "contexts": contexts,
            "ground_truth": ground_truths,
        })
        
        return dataset
    
    def evaluate(self, dataset: Dataset) -> EvaluationResult:
        """
        æ‰§è¡Œ RAGAS è¯„æµ‹
        
        ä½¿ç”¨ RAGAS æ¡†æ¶è®¡ç®—ä»¥ä¸‹æŒ‡æ ‡ï¼š
        - Faithfulnessï¼ˆå¿ å®åº¦ï¼‰
        - Answer Relevancyï¼ˆç­”æ¡ˆç›¸å…³æ€§ï¼‰
        - Context Precisionï¼ˆä¸Šä¸‹æ–‡ç²¾ç¡®åº¦ï¼‰
        - Context Recallï¼ˆä¸Šä¸‹æ–‡å¬å›ç‡ï¼‰
        
        Args:
            dataset: å‡†å¤‡å¥½çš„è¯„æµ‹æ•°æ®é›†
            
        Returns:
            EvaluationResult åŒ…å«å„é¡¹æŒ‡æ ‡åˆ†æ•°
            
        Validates:
            - Requirement 5.2: è®¡ç®— Faithfulness æŒ‡æ ‡è¯„ä¼°ç­”æ¡ˆå¿ å®åº¦
            - Requirement 5.3: è®¡ç®— Answer_Relevancy æŒ‡æ ‡è¯„ä¼°ç­”æ¡ˆç›¸å…³æ€§
            - Requirement 5.4: è®¡ç®— Context_Precision æŒ‡æ ‡è¯„ä¼°ä¸Šä¸‹æ–‡ç²¾ç¡®åº¦
            - Requirement 5.5: è®¡ç®— Context_Recall æŒ‡æ ‡è¯„ä¼°ä¸Šä¸‹æ–‡å¬å›ç‡
        """
        # è°ƒç”¨ RAGAS evaluate å‡½æ•°ï¼Œä½¿ç”¨è‡ªå®šä¹‰çš„ LLM å’Œ Embeddings
        # Validates Requirements 5.2, 5.3, 5.4, 5.5
        result = evaluate(
            dataset=dataset,
            metrics=self.metrics,
            llm=self._get_llm(),
            embeddings=self._get_embeddings(),
        )
        
        # æå–å„é¡¹æŒ‡æ ‡åˆ†æ•°
        # RAGAS è¿”å›çš„ç»“æœå¯èƒ½æ˜¯å­—å…¸æˆ–å¯¹è±¡ï¼Œéœ€è¦å…¼å®¹å¤„ç†
        faithfulness_score = 0.0
        answer_relevancy_score = 0.0
        context_precision_score = 0.0
        context_recall_score = 0.0
        
        # å°è¯•ä»ä¸åŒçš„å±æ€§/æ–¹æ³•è·å–åˆ†æ•°
        if hasattr(result, '__getitem__'):
            # å­—å…¸å½¢å¼è®¿é—®
            try:
                val = result["faithfulness"]
                faithfulness_score = float(val) if val is not None else 0.0
                val = result["answer_relevancy"]
                answer_relevancy_score = float(val) if val is not None else 0.0
                val = result["context_precision"]
                context_precision_score = float(val) if val is not None else 0.0
                val = result["context_recall"]
                context_recall_score = float(val) if val is not None else 0.0
            except (KeyError, TypeError, ValueError):
                pass
        
        # å¦‚æœä¸Šé¢æ²¡æœ‰è·å–åˆ°ï¼Œå°è¯•å…¶ä»–æ–¹å¼
        if faithfulness_score == 0.0 and answer_relevancy_score == 0.0:
            # å°è¯•ä» to_pandas è·å–å¹³å‡å€¼
            try:
                df = result.to_pandas()
                if "faithfulness" in df.columns:
                    val = df["faithfulness"].mean()
                    faithfulness_score = float(val) if val is not None and not (isinstance(val, float) and val != val) else 0.0
                if "answer_relevancy" in df.columns:
                    val = df["answer_relevancy"].mean()
                    answer_relevancy_score = float(val) if val is not None and not (isinstance(val, float) and val != val) else 0.0
                if "context_precision" in df.columns:
                    val = df["context_precision"].mean()
                    context_precision_score = float(val) if val is not None and not (isinstance(val, float) and val != val) else 0.0
                if "context_recall" in df.columns:
                    val = df["context_recall"].mean()
                    context_recall_score = float(val) if val is not None and not (isinstance(val, float) and val != val) else 0.0
            except Exception:
                pass
        
        # æå–è¯¦ç»†åˆ†æ•°ï¼ˆæ¯ä¸ªæ ·æœ¬çš„åˆ†æ•°ï¼‰
        details = {}
        try:
            df = result.to_pandas()
            details = df.to_dict(orient="records")
        except Exception:
            # å¦‚æœæ— æ³•è·å–è¯¦ç»†åˆ†æ•°ï¼Œä½¿ç”¨ç©ºå­—å…¸
            details = {}
        
        # æ„å»ºå¹¶è¿”å› EvaluationResult
        return EvaluationResult(
            faithfulness=faithfulness_score,
            answer_relevancy=answer_relevancy_score,
            context_precision=context_precision_score,
            context_recall=context_recall_score,
            details=details,
        )
    
    def generate_report(self, result: EvaluationResult) -> str:
        """
        ç”Ÿæˆè¯„æµ‹æŠ¥å‘Š
        
        ç”Ÿæˆæ ¼å¼åŒ–çš„è¯„æµ‹æŠ¥å‘Šï¼ŒåŒ…å«æ‰€æœ‰æŒ‡æ ‡çš„åˆ†æ•°å’Œè§£é‡Šã€‚
        
        Args:
            result: è¯„æµ‹ç»“æœ
            
        Returns:
            æ ¼å¼åŒ–çš„è¯„æµ‹æŠ¥å‘Šå­—ç¬¦ä¸²
            
        Validates:
            - Requirement 5.6: è¾“å‡ºåŒ…å«æ‰€æœ‰æŒ‡æ ‡çš„è¯„æµ‹æŠ¥å‘Š
        """
        # æ„å»ºæŠ¥å‘Šæ ‡é¢˜
        report_lines = [
            "=" * 60,
            "RAGAS è¯„æµ‹æŠ¥å‘Š",
            "=" * 60,
            "",
            "ğŸ“Š è¯„æµ‹æŒ‡æ ‡æ€»è§ˆ",
            "-" * 40,
            "",
        ]
        
        # æ·»åŠ å„é¡¹æŒ‡æ ‡åˆ†æ•°
        # Validates Requirement 5.6: è¾“å‡ºåŒ…å«æ‰€æœ‰æŒ‡æ ‡çš„è¯„æµ‹æŠ¥å‘Š
        report_lines.extend([
            f"ğŸ¯ Faithfulnessï¼ˆå¿ å®åº¦ï¼‰: {result.faithfulness:.4f}",
            f"   - è¡¡é‡ç”Ÿæˆç­”æ¡ˆä¸æ£€ç´¢ä¸Šä¸‹æ–‡çš„ä¸€è‡´æ€§",
            "",
            f"ğŸ“ Answer Relevancyï¼ˆç­”æ¡ˆç›¸å…³æ€§ï¼‰: {result.answer_relevancy:.4f}",
            f"   - è¡¡é‡ç­”æ¡ˆä¸é—®é¢˜çš„ç›¸å…³ç¨‹åº¦",
            "",
            f"ğŸ” Context Precisionï¼ˆä¸Šä¸‹æ–‡ç²¾ç¡®åº¦ï¼‰: {result.context_precision:.4f}",
            f"   - è¡¡é‡æ£€ç´¢ä¸Šä¸‹æ–‡çš„ç²¾ç¡®æ€§",
            "",
            f"ğŸ“š Context Recallï¼ˆä¸Šä¸‹æ–‡å¬å›ç‡ï¼‰: {result.context_recall:.4f}",
            f"   - è¡¡é‡æ£€ç´¢ä¸Šä¸‹æ–‡çš„å®Œæ•´æ€§",
            "",
        ])
        
        # æ·»åŠ ç»¼åˆè¯„åˆ†
        avg_score = (
            result.faithfulness +
            result.answer_relevancy +
            result.context_precision +
            result.context_recall
        ) / 4
        
        report_lines.extend([
            "-" * 40,
            f"ğŸ“ˆ ç»¼åˆè¯„åˆ†: {avg_score:.4f}",
            "",
        ])
        
        # æ·»åŠ è¯„åˆ†è§£è¯»
        report_lines.extend([
            "ğŸ“‹ è¯„åˆ†è§£è¯»",
            "-" * 40,
        ])
        
        if avg_score >= 0.8:
            report_lines.append("âœ… ä¼˜ç§€ï¼šRAG ç³»ç»Ÿè¡¨ç°å‡ºè‰²ï¼Œå„é¡¹æŒ‡æ ‡å‡å¤„äºè¾ƒé«˜æ°´å¹³ã€‚")
        elif avg_score >= 0.6:
            report_lines.append("ğŸ”¶ è‰¯å¥½ï¼šRAG ç³»ç»Ÿè¡¨ç°è‰¯å¥½ï¼Œä½†ä»æœ‰æå‡ç©ºé—´ã€‚")
        elif avg_score >= 0.4:
            report_lines.append("âš ï¸ ä¸€èˆ¬ï¼šRAG ç³»ç»Ÿè¡¨ç°ä¸€èˆ¬ï¼Œå»ºè®®ä¼˜åŒ–æ£€ç´¢å’Œç”Ÿæˆç­–ç•¥ã€‚")
        else:
            report_lines.append("âŒ éœ€æ”¹è¿›ï¼šRAG ç³»ç»Ÿè¡¨ç°è¾ƒå·®ï¼Œéœ€è¦é‡ç‚¹ä¼˜åŒ–ã€‚")
        
        report_lines.extend([
            "",
            "=" * 60,
        ])
        
        return "\n".join(report_lines)
    
    def run_evaluation(self, dataset_path: str) -> tuple[EvaluationResult, str]:
        """
        è¿è¡Œå®Œæ•´çš„è¯„æµ‹æµç¨‹
        
        ä¾¿æ·æ–¹æ³•ï¼Œä¾æ¬¡æ‰§è¡Œï¼šåŠ è½½æ•°æ®é›† -> å‡†å¤‡è¯„æµ‹æ•°æ® -> æ‰§è¡Œè¯„æµ‹ -> ç”ŸæˆæŠ¥å‘Š
        
        Args:
            dataset_path: è¯„æµ‹æ•°æ®é›†æ–‡ä»¶è·¯å¾„
            
        Returns:
            å…ƒç»„ (EvaluationResult, æŠ¥å‘Šå­—ç¬¦ä¸²)
        """
        # 1. åŠ è½½æ•°æ®é›†
        samples = self.load_dataset(dataset_path)
        
        # 2. å‡†å¤‡è¯„æµ‹æ•°æ®
        dataset = self.prepare_evaluation_data(samples)
        
        # 3. æ‰§è¡Œè¯„æµ‹
        result = self.evaluate(dataset)
        
        # 4. ç”ŸæˆæŠ¥å‘Š
        report = self.generate_report(result)
        
        return result, report
